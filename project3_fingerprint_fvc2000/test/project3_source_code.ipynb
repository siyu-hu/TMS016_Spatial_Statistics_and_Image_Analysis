{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee9c47c",
   "metadata": {},
   "source": [
    "# Project 3 - Fingerprint verification\n",
    "\n",
    "[GitHub](https://github.com/siyu-hu/TMS016_Spatial_Statistics_and_Image_Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb9f642",
   "metadata": {},
   "source": [
    "# Preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d79ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unified fingerprint-image preprocessing module\n",
    "Key features\n",
    "1. Advanced enhancement pipeline\n",
    "   Gamma correction → Zero-mean / unit-variance gray normalization →\n",
    "   Orientation field estimation → Overlapping-block Gabor filtering → CLAHE\n",
    "\n",
    "2. Backward-compatible public entry point*\n",
    "   `normalize(image_path) → float32 array [300 * 300], range 0-1`\n",
    "\n",
    "3. Batch processing \n",
    "   Saves two copies simultaneously  \n",
    "     • Enhanced **tif** → ./data/original/<*_new>  \n",
    "     • Training **npy**  → ./data/processed/<*_new>\n",
    "\n",
    "What changed compared with the old version?\n",
    "-------------------------------------------\n",
    "✓ **CHANGED** - The former “simple” normalization was replaced; the external\n",
    "                calling method stays the same.  \n",
    "✓ **NEW**      - `normalize_gray`, core preprocessing functions,\n",
    "                and a re-worked `batch_preprocess`.  \n",
    "✓ **KEPT**     - `check_image_sizes` \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "\n",
    "def check_image_sizes(folder_path, expected_size=(300, 300)):\n",
    "    \"\"\"Iterate through tif files and report those whose size is unexpected.\"\"\"\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith('.tif'):\n",
    "            path = os.path.join(folder_path, fname)\n",
    "            img  = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                print(f\" ERROR:  Failed to read {fname}\")\n",
    "                continue\n",
    "            if img.shape != expected_size:\n",
    "                print(f\"  {fname} has size {img.shape} ≠ {expected_size}\")\n",
    "\n",
    "# ------------------------------------------------------------------------- #\n",
    "#                      advanced pipeline by Qi Wang                     #\n",
    "# ------------------------------------------------------------------------- #\n",
    "def normalize_gray(img):\n",
    "    \"\"\"Zero-mean / unit-variance, then stretch to 0–255 (uint8).\"\"\"\n",
    "    mean, std = img.mean(), img.std()\n",
    "    z = (img - mean) / (std + 1e-5)\n",
    "    z = (z - z.min()) / (z.max() - z.min()) * 255\n",
    "    return z.astype(np.uint8)\n",
    "\n",
    "def gamma_correction(img, gamma=0.3):\n",
    "    inv = 1.0 / gamma\n",
    "    table = ((np.arange(256) / 255.0) ** inv * 255).astype(\"uint8\")\n",
    "    return cv2.LUT(img, table)\n",
    "\n",
    "def compute_orientation_field(img, block_size=16):\n",
    "    rows, cols = img.shape\n",
    "    sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    orient = np.zeros((rows // block_size, cols // block_size))\n",
    "\n",
    "    for i in range(0, rows - block_size, block_size):\n",
    "        for j in range(0, cols - block_size, block_size):\n",
    "            gx = sobelx[i:i+block_size, j:j+block_size]\n",
    "            gy = sobely[i:i+block_size, j:j+block_size]\n",
    "            Vx = 2 * np.sum(gx * gy)\n",
    "            Vy = np.sum(gx**2 - gy**2)\n",
    "            orient[i//block_size, j//block_size] = 0.5 * np.arctan2(Vx, Vy)\n",
    "    return orient\n",
    "\n",
    "def gabor_filter_overlap(img, orientation_field,\n",
    "                         block_size=16, freq=0.1, sigma=4.0):\n",
    "    rows, cols = img.shape\n",
    "    enhanced = np.zeros((rows, cols), np.float32)\n",
    "    weights  = np.zeros((rows, cols), np.float32)\n",
    "    stride   = block_size // 2\n",
    "\n",
    "    for i in range(0, rows - block_size, stride):\n",
    "        for j in range(0, cols - block_size, stride):\n",
    "            theta  = orientation_field[i//block_size, j//block_size]\n",
    "            kernel = cv2.getGaborKernel((block_size, block_size),\n",
    "                                        sigma, theta, 1.0/freq, 0.5, 0,\n",
    "                                        ktype=cv2.CV_32F)\n",
    "            block    = img[i:i+block_size, j:j+block_size]\n",
    "            filtered = cv2.filter2D(block, cv2.CV_32F, kernel)\n",
    "\n",
    "            enhanced[i:i+block_size, j:j+block_size] += filtered\n",
    "            weights[i:i+block_size, j:j+block_size]  += 1.0\n",
    "\n",
    "    enhanced /= np.where(weights == 0, 1.0, weights)\n",
    "    return cv2.normalize(enhanced, None, 0, 255,\n",
    "                         cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "def apply_clahe(img, clipLimit=3.0, tileGridSize=(8, 8)):\n",
    "    \"\"\"Local contrast enhancement (CLAHE).\"\"\"\n",
    "    clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n",
    "    return clahe.apply(img)\n",
    "\n",
    "def preprocess_fingerprint(img_path):\n",
    "    \"\"\"\n",
    "    Full preprocessing of a single image (returns uint8, original size).\n",
    "    Gamma → normalize_gray → orientation field → Gabor → CLAHE\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"ERROR: Cannot open {img_path}\")\n",
    "\n",
    "    stage1 = gamma_correction(img)\n",
    "    stage2 = normalize_gray(stage1)\n",
    "    field  = compute_orientation_field(stage2)\n",
    "    gabor  = gabor_filter_overlap(stage2, field)\n",
    "    return apply_clahe(gabor)\n",
    "\n",
    "\n",
    "def normalize(image_path, size=(300, 300)):\n",
    "    \"\"\"\n",
    "    Backward-compatible wrapper.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_path : str\n",
    "        Path to a grayscale fingerprint image.\n",
    "    size : tuple, default (300, 300)\n",
    "        Output spatial size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray, float32, shape = size\n",
    "        Pixel range 0 - 1, ready for the network.\n",
    "    \"\"\"\n",
    "    img = preprocess_fingerprint(image_path)          # uint8\n",
    "    img = cv2.resize(img, size)\n",
    "    return img.astype('float32') / 255.0\n",
    "\n",
    "# ------------------------------------------------------------------------- #\n",
    "#       Batch processing (save tif + npy to the specified locations)        #\n",
    "# ------------------------------------------------------------------------- #\n",
    "def batch_preprocess(input_dir,\n",
    "                     output_dir_npy=\"./data/processed/DB1_B_new_1\",\n",
    "                     output_dir_tif=\"./data/original/DB1_B_new_1\",\n",
    "                     size=(300, 300)):\n",
    "    \"\"\"\n",
    "    Convert all tif/bmp/jpg/png in `input_dir`.\n",
    "    • Enhanced tif (uint8, 0 - 255) → `output_dir_tif`\n",
    "    • Training npy (float32, 0 - 1) → `output_dir_npy`\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir_npy, exist_ok=True)\n",
    "    os.makedirs(output_dir_tif, exist_ok=True)\n",
    "\n",
    "    count = 0\n",
    "    for fname in tqdm(os.listdir(input_dir), desc=\"preprocessing\"):\n",
    "        if fname.lower().endswith(('.tif', '.bmp', '.jpg', '.png')):\n",
    "            in_path = os.path.join(input_dir, fname)\n",
    "            try:\n",
    "                img = preprocess_fingerprint(in_path)\n",
    "                img = cv2.resize(img, size)\n",
    "\n",
    "                stem = os.path.splitext(fname)[0]\n",
    "                # Save tif\n",
    "                cv2.imwrite(os.path.join(output_dir_tif, f\"{stem}.tif\"), img)\n",
    "                # Save npy (float32, 0–1)\n",
    "                np.save(os.path.join(output_dir_npy, f\"{stem}.npy\"),\n",
    "                        img.astype('float32') / 255.0)\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\" {fname}: {e}\")\n",
    "\n",
    "    print(f\"\\n {count} images saved to\")\n",
    "    print(f\"   tif : {output_dir_tif}\")\n",
    "    print(f\"   npy : {output_dir_npy}\")\n",
    "\n",
    "# ------------------------------------------------------------------------- #\n",
    "#                                  CLI                                    #\n",
    "# ------------------------------------------------------------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Fingerprint batch preprocessing (advanced pipeline)\")\n",
    "    parser.add_argument(\"--input\", \"-i\", required=True,\n",
    "                        help=\"Directory containing raw fingerprint images\")\n",
    "    parser.add_argument(\"--out-npy\", default=\"./data/processed/DB1_B_new_1\",\n",
    "                        help=\"Destination folder for processed .npy files\")\n",
    "    parser.add_argument(\"--out-tif\", default=\"./data/original/DB1_B_new_1\",\n",
    "                        help=\"Destination folder for enhanced .tif files\")\n",
    "    parser.add_argument(\"--size\", type=int, default=300,\n",
    "                        help=\"Output width/height after resize (default: 300)\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    batch_preprocess(input_dir=args.input,\n",
    "                     output_dir_npy=args.out_npy,\n",
    "                     output_dir_tif=args.out_tif,\n",
    "                     size=(args.size, args.size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61dead3",
   "metadata": {},
   "source": [
    "# augment_images.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ad85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "def random_affine_transform(image, max_rotation=10, max_scale=0.1):\n",
    "    \"\"\"\n",
    "    Apply random affine transformation (small rotation + scaling) to a single image.\n",
    "\n",
    "    Args:\n",
    "        image: numpy array, single-channel (H, W)\n",
    "        max_rotation: maximum rotation angle (± degrees)\n",
    "        max_scale: maximum scale variation (± percentage)\n",
    "\n",
    "    Returns:\n",
    "        Transformed image with the same size.\n",
    "    \"\"\"\n",
    "\n",
    "    h, w = image.shape\n",
    "\n",
    "    # 1. Random rotation angle\n",
    "    angle = random.uniform(-max_rotation, max_rotation)\n",
    "\n",
    "    # 2. Random scaling factor\n",
    "    scale = 1.0 + random.uniform(-max_scale, max_scale)\n",
    "\n",
    "    # 3. Build the affine transformation matrix\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "\n",
    "    # 4. Apply the affine transformation\n",
    "    transformed = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
    "\n",
    "    return transformed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce64e018",
   "metadata": {},
   "source": [
    "# create_train_pairs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import combinations\n",
    "from augment_images import random_affine_transform\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def load_images_by_finger(data_path):\n",
    "    \"\"\"key = finger_id; value = image path list\"\"\"\n",
    "    finger_dict = {}\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith('.npy'):\n",
    "            finger_id = file.split('_')[0]\n",
    "            finger_dict.setdefault(finger_id, []).append(os.path.join(data_path, file))\n",
    "    for fid in finger_dict:\n",
    "        finger_dict[fid] = sorted(finger_dict[fid])\n",
    "    return finger_dict\n",
    "\n",
    "def load_images_by_finger_tif(data_path):\n",
    "    \"\"\"load tif (for DB3_B test images)\"\"\"\n",
    "    finger_dict = {}\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.lower().endswith('.tif'):\n",
    "            finger_id = file.split('_')[0]\n",
    "            finger_dict.setdefault(finger_id, []).append(os.path.join(data_path, file))\n",
    "    for fid in finger_dict:\n",
    "        finger_dict[fid] = sorted(finger_dict[fid])\n",
    "    return finger_dict\n",
    "\n",
    "\n",
    "def create_pairs(finger_dict, selected_fingers, augment_positive=False, num_augments=2,balance_negatives=False):\n",
    "    \"\"\"\n",
    "    Create positive and negative pairs.\n",
    "    If augment_positive=True, perform data augmentation on positive pairs.\n",
    "    If balance_negatives=True, sample negative pairs to match positive pairs count.\n",
    "    \n",
    "    Args:\n",
    "        finger_dict: dict of {finger_id: list of npy file paths}\n",
    "        selected_fingers: list of selected finger ids\n",
    "        augment_positive: whether to augment positive pairs\n",
    "        num_augments: how many augmentations per positive pair\n",
    "        balance_negatives: whether to balance negative samples\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    for fid in selected_fingers:\n",
    "        images = finger_dict[fid]\n",
    "\n",
    "        # postive pairs (same finger)\n",
    "        for img1, img2 in combinations(images, 2):\n",
    "            img1 = os.path.relpath(img1, start=\".\")  \n",
    "            img2 = os.path.relpath(img2, start=\".\")\n",
    "            pairs.append([img1, img2])\n",
    "            labels.append(1) # label = 1 ->> positive pair (same finger)\n",
    "\n",
    "            if augment_positive:\n",
    "                # add data augmentation for positive pairs\n",
    "                img1_arr = np.load(img1)\n",
    "                img2_arr = np.load(img2)\n",
    "\n",
    "                for _ in range(num_augments):\n",
    "                    aug1 = random_affine_transform(img1_arr)\n",
    "                    aug2 = random_affine_transform(img2_arr)\n",
    "\n",
    "                    # save temporary augmented images in memory\n",
    "                    pairs.append([aug1, aug2])\n",
    "                    labels.append(1)\n",
    "        \n",
    "    num_positive = sum(1 for l in labels if l == 1)\n",
    "\n",
    "    # negative pairs (different fingers, randomly selected)\n",
    "    negative_pairs = []\n",
    "    all_fingers = list(selected_fingers)\n",
    "    for i in range(len(all_fingers)):\n",
    "        for j in range(i+1, len(all_fingers)):\n",
    "            imgs1 = finger_dict[all_fingers[i]]\n",
    "            imgs2 = finger_dict[all_fingers[j]]\n",
    "            for img1 in imgs1:\n",
    "                for img2 in imgs2:\n",
    "                    img1 = os.path.relpath(img1, start=\".\")  \n",
    "                    img2 = os.path.relpath(img2, start=\".\")\n",
    "                    negative_pairs.append(([img1, img2], 0))\n",
    "\n",
    "    if balance_negatives:\n",
    "        # Randomly sample same number of negative pairs as positive pairs\n",
    "        negative_pairs = random.sample(negative_pairs, min(num_positive, len(negative_pairs)))\n",
    "\n",
    "    # Add negative pairs\n",
    "    for pair, label in negative_pairs:\n",
    "        pairs.append(pair)\n",
    "        labels.append(label)\n",
    "\n",
    "    pairs, labels = shuffle(pairs, labels, random_state=42) \n",
    "    return pairs, labels\n",
    "\n",
    "\n",
    "\n",
    "def save_pairs(pairs, labels, output_file):\n",
    "    pairs = np.array(pairs, dtype=object)  # [img1_path, img2_path]\n",
    "    labels = np.array(labels)\n",
    "    np.savez(output_file, pairs=pairs, labels=labels)\n",
    "    print(f\"Saved {len(pairs)} pairs to {output_file}.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c45ab5",
   "metadata": {},
   "source": [
    "# siamese_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c35c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# siamese.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight Siamese CNN for 300 * 300 grayscale fingerprints.\n",
    "\n",
    "    Input  : two tensors of shape [B, 1, 300, 300]\n",
    "    Output : two L2-normalised embedding tensors of shape [B, embedding_dim]\n",
    "\n",
    "    Total parameters ≈ 0.11 M (vs. ≈ 90 M in the original design).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        # -------- Convolutional feature extractor --------\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),   # [B, 16, 300, 300]\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),                  # [B, 16, 150, 150]\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # [B, 32, 150, 150]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),                  # [B, 32, 75, 75]\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # [B, 64, 75, 75]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3),                  # [B, 64, 25, 25]\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # [B, 128, 25, 25]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1, 1))                  # [B, 128, 1, 1]\n",
    "        )\n",
    "\n",
    "        # -------- Projection to low-dimensional embedding --------\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Flatten(),                                 # [B, 128]\n",
    "            nn.Linear(128, embedding_dim)                 # [B, embedding_dim]\n",
    "        )\n",
    "\n",
    "    # Forward pass for one branch\n",
    "    def forward_once(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.projection(x)\n",
    "        x = F.normalize(x, p=2, dim=1)                   # L2 normalisation\n",
    "        return x\n",
    "\n",
    "    # Siamese forward: return embeddings for both inputs\n",
    "    def forward(self, x1: torch.Tensor, x2: torch.Tensor):\n",
    "        return self.forward_once(x1), self.forward_once(x2)\n",
    "\n",
    "\n",
    "# ------------- quick self-test -------------\n",
    "if __name__ == \"__main__\":\n",
    "    net = SiameseNetwork()\n",
    "    dummy_a = torch.randn(4, 1, 300, 300)   # batch = 4\n",
    "    dummy_b = torch.randn(4, 1, 300, 300)\n",
    "    emb_a, emb_b = net(dummy_a, dummy_b)\n",
    "    print(emb_a.shape, emb_b.shape)         # torch.Size([4, 128]) torch.Size([4, 128])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e81e4",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849fb9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from siamese_model import SiameseNetwork\n",
    "from utils import SiameseDataset, ContrastiveLoss, plot_loss, save_checkpoint\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "# --- add CLI -------------------------------------------\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--train_pairs\", default=None)\n",
    "parser.add_argument(\"--val_pairs\",   default=None)\n",
    "parser.add_argument(\"--finetune\", action=\"store_true\",\n",
    "                    help=\"Continue training from --best_ckpt with typically fewer epochs / lower lr\")\n",
    "\n",
    "parser.add_argument(\"--lr\",          type=float)\n",
    "parser.add_argument(\"--num_epochs\",  type=int)\n",
    "parser.add_argument(\"--use_aug\",     action=\"store_true\")  \n",
    "parser.add_argument(\"--balance_neg\", action=\"store_true\")  \n",
    "parser.add_argument(\"--best_ckpt\",   default=None)   \n",
    "args = parser.parse_args()\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def train_one_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for img1, img2, label in tqdm(dataloader, desc=\"Training\", leave=False): \n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img1, img2)\n",
    "        loss = loss_fn(output1, output2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, label in tqdm(dataloader, desc=\"Validating\", leave=False):\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = loss_fn(output1, output2, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # -------------- Config --------------\n",
    "    use_augmentation = args.use_aug # IMPORTANT: Set to True if training on augmented data\n",
    "    balance_negatives = args.balance_neg  # IMPORTANT: Should match how you created training pairs\n",
    "    finetune = args.finetune    # IMPORTANT: Set to True to continue training from the best checkpoint\n",
    "\n",
    "\n",
    "    if use_augmentation:\n",
    "        default_train_path = \"./data/new_train_pairs_augmented.npz\"\n",
    "    else:\n",
    "        default_train_path = \"./data/new_train_pairs.npz\"\n",
    "\n",
    "    train_data_path = args.train_pairs or default_train_path\n",
    "    val_data_path   = args.val_pairs or \"./data/new_val_pairs.npz\"\n",
    "\n",
    "\n",
    "    batch_size = 8\n",
    "    margin = 2.0\n",
    "\n",
    "    if args.finetune:                \n",
    "        default_lr     = 1e-4\n",
    "        default_epoch  = 10\n",
    "    else:                            \n",
    "        default_lr     = 5e-4\n",
    "        default_epoch  = 20\n",
    "\n",
    "    learning_rate = args.lr if args.lr is not None else default_lr\n",
    "    num_epochs    = args.num_epochs if args.num_epochs is not None else default_epoch\n",
    "\n",
    "    # output ckpt \n",
    "    ckpt_filename = f\"new_model_ft{finetune}_aug{use_augmentation}_bl{balance_negatives}_bs{batch_size}_ep{num_epochs}_lr{learning_rate}_mg{margin}.pt\"\n",
    "    ckpt_path = f\"./checkpoints/{ckpt_filename}\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    best_ckpt_path = args.best_ckpt or ckpt_path\n",
    "    # -------------- Dataset + Dataloader -------------\n",
    "    train_dataset = SiameseDataset(train_data_path, root_dir=\".\")\n",
    "    val_dataset = SiameseDataset(val_data_path, root_dir=\".\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # ------------ Model, Loss, Optimizer --------------\n",
    "    model = SiameseNetwork().to(device)\n",
    "\n",
    "    if finetune and os.path.exists(best_ckpt_path): \n",
    "        print(f\" Continue training from checkpoint: {best_ckpt_path}\")\n",
    "        model.load_state_dict(torch.load(best_ckpt_path, map_location=device))\n",
    "        print(\"[INFO] Fine-tuning mode: freezing convolutional backbone...\")\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False # freeze conv layers, only train the fc layer\n",
    "\n",
    "    loss_fn = ContrastiveLoss(margin=margin)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate) \n",
    "\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.1, patience=2, verbose=True)\n",
    "    early_stop_patience = 7      # if validation loss does not improve for this many epochs, stop training\n",
    "    bad_epochs = 0\n",
    "\n",
    "    # ------ Training Loop -----------------\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n Epoch {epoch+1}/{num_epochs}\")\n",
    "        train_loss = train_one_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "        val_loss = validate(model, val_loader, loss_fn, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(model, ckpt_path)\n",
    "            print(f\"Saved improved model to {ckpt_path}\")\n",
    "            bad_epochs = 0           # reset\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "        # early stopping\n",
    "        if bad_epochs >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    plot_loss(train_losses, val_losses, save_path=\"./outputs/loss_curve.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff234917",
   "metadata": {},
   "source": [
    "# validate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6150d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from siamese_model import SiameseNetwork\n",
    "from utils import SiameseDataset, print_classification_report\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils import plot_distance_distribution, plot_roc_curve, plot_metrics_vs_threshold\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_accuracy(model, dataloader, threshold=0.5, device=\"cpu\"):\n",
    "    print(f\"\\n Threshold used: {threshold}\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tp = tn = fp = fn = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, label in tqdm(dataloader, desc=\"Validating\"):\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "\n",
    "            out1, out2 = model(img1, img2)\n",
    "            distance = F.pairwise_distance(out1, out2)\n",
    "\n",
    "            # distance < threshold means similar (1), distance >= threshold means dissimilar (0)\n",
    "            prediction = (distance < threshold).float()\n",
    "\n",
    "            correct += (prediction == label).sum().item()\n",
    "            total += label.size(0)\n",
    "\n",
    "            tp += ((prediction == 1) & (label == 1)).sum().item()\n",
    "            tn += ((prediction == 0) & (label == 0)).sum().item()\n",
    "            fp += ((prediction == 1) & (label == 0)).sum().item()\n",
    "            fn += ((prediction == 0) & (label == 1)).sum().item()\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print_classification_report(tp, tn, fp, fn)\n",
    "    return accuracy\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--val_data\", required=True, help=\"Path to validation pairs npz file\")\n",
    "    parser.add_argument(\"--ckpt\", required=True, help=\"Path to checkpoint file\", default=\"./checkpoints/model_augTrue_blTrue_bs8_ep20_lr0.001_mg2.0.pt\")\n",
    "    parser.add_argument(\"--threshold\", type=float, default=1.010204, help=\"Threshold for distance\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.makedirs(\"./outputs\", exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    batch_size = 8\n",
    "\n",
    "    # Load the best threshold from file if it exists\n",
    "    # threshold_file = \"./project3_fingerprint_fvc2000/outputs/best_threshold.txt\"\n",
    "    # if os.path.exists(threshold_file):\n",
    "    #     with open(threshold_file, \"r\") as f:\n",
    "    #         threshold = float(f.read().strip())\n",
    "    #     print(f\"Loaded best threshold: {threshold}\")\n",
    "    # else:\n",
    "    #     threshold = 0.05  # fallback \n",
    "    #     print(f\"No threshold file found, using default threshold = {threshold}\")\n",
    "\n",
    "    # IMPORTANT: Change the model path to your trained model\n",
    "    ckpt_path = args.ckpt\n",
    "\n",
    "    model = SiameseNetwork().to(device)\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    print(f\"Loaded model from {ckpt_path}\")\n",
    "\n",
    "    val_dataset = SiameseDataset(args.val_data, root_dir=\".\")\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    evaluate_accuracy(model, val_loader, threshold=args.threshold, device=device)\n",
    "\n",
    "    plot_distance_distribution(model, val_loader, device, save_path=\"./outputs/distance_hist.png\")\n",
    "    plot_roc_curve(model, val_loader, device, save_path=\"./outputs/roc_curve.png\")\n",
    "    plot_metrics_vs_threshold(model, val_loader, device, save_path=\"./outputs/metrics_vs_threshold.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868423f",
   "metadata": {},
   "source": [
    "# inference_batch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d7627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from project3_fingerprint_fvc2000.preprocess_old import normalize\n",
    "from create_train_pairs import load_images_by_finger_tif, create_pairs\n",
    "from siamese_model import SiameseNetwork\n",
    "from utils import print_classification_report\n",
    "from random import sample\n",
    "import argparse\n",
    "\n",
    "\n",
    "def inference_batch(model, pairs, labels, threshold=0.041, device=\"cpu\", desc=\"Inferencing\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    tp = tn = fp = fn = 0\n",
    "\n",
    "    for (img1_path, img2_path), label in tqdm(zip(pairs, labels), total=len(pairs), desc=desc):\n",
    "        img1 = normalize(img1_path)\n",
    "        img2 = normalize(img2_path)\n",
    "\n",
    "        img1 = torch.from_numpy(img1).unsqueeze(0).unsqueeze(0).to(device)  # shape: [1, 1, H, W]\n",
    "        img2 = torch.from_numpy(img2).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        label = torch.tensor(label).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out1, out2 = model(img1, img2)\n",
    "            dist = F.pairwise_distance(out1, out2).item()\n",
    "            prediction = 1.0 if dist < threshold else 0.0\n",
    "\n",
    "            correct += (prediction == label.item())\n",
    "            total += 1\n",
    "\n",
    "            # Update confusion matrix\n",
    "            if prediction == 1.0 and label.item() == 1:\n",
    "                tp += 1\n",
    "            elif prediction == 0.0 and label.item() == 0:\n",
    "                tn += 1\n",
    "            elif prediction == 1.0 and label.item() == 0:\n",
    "                fp += 1\n",
    "            elif prediction == 0.0 and label.item() == 1:\n",
    "                fn += 1\n",
    "\n",
    "    acc = correct / total * 100\n",
    "    print_classification_report(tp, tn, fp, fn)\n",
    "\n",
    "\n",
    "def auto_calibrate_threshold(model, calib_pairs, calib_labels, device=\"cpu\",\n",
    "                             search_min=0.0, search_max=2.0, steps=120):\n",
    "    \"\"\"\n",
    "    Given a batch of calibrated pairs + labels, scan the threshold to find the highest F1 point.\n",
    "    Returns: best_threshold, best_f1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dists = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (p1, p2), _ in zip(calib_pairs, calib_labels):\n",
    "            a_arr = normalize(p1)   \n",
    "            b_arr = normalize(p2)\n",
    "            a = torch.from_numpy(a_arr).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            b = torch.from_numpy(b_arr).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            f1, f2 = model(a, b)\n",
    "            dists.append(F.pairwise_distance(f1, f2).item())\n",
    "\n",
    "    dists = np.array(dists)\n",
    "    labs  = np.array(calib_labels)\n",
    "\n",
    "    thresholds = np.linspace(search_min, search_max, steps)\n",
    "    best_f1, best_t = 0.0, thresholds[0]\n",
    "\n",
    "    for t in thresholds:\n",
    "        pred = (dists < t).astype(int)\n",
    "        tp = np.sum((pred == 1) & (labs == 1))\n",
    "        fp = np.sum((pred == 1) & (labs == 0))\n",
    "        fn = np.sum((pred == 0) & (labs == 1))\n",
    "        prec = tp / (tp + fp + 1e-8)\n",
    "        rec  = tp / (tp + fn + 1e-8)\n",
    "        f1   = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "\n",
    "    return best_t, best_f1\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--inference_data\", type=str, required=True, help=\"Path to inference images folder\")\n",
    "    parser.add_argument(\"--ckpt\", type=str, required=True, help=\"Path to model checkpoint\")\n",
    "    parser.add_argument(\"--thresholds\", nargs='+', type=float,\n",
    "                    help=\"List of thresholds to try, e.g., 0.83 0.85 0.87 0.90\")\n",
    "    parser.add_argument(\"--auto_threshold\", action=\"store_true\",\n",
    "                    help=\"Use auto-calibrated threshold (20% data for calibration)\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # ------------ paths ------------\n",
    "    inference_data_path = args.inference_data\n",
    "    model_path = args.ckpt\n",
    "    \n",
    "\n",
    "    # ------------ device / model ------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SiameseNetwork().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "    # ------------ build pairs for this DB ------------\n",
    "    finger_dict = load_images_by_finger_tif(inference_data_path)\n",
    "    pairs, labels = create_pairs(finger_dict, sorted(finger_dict.keys()),\n",
    "                                 augment_positive=False, num_augments=0,\n",
    "                                 balance_negatives=False)\n",
    "    \n",
    "    \n",
    "    if args.auto_threshold:\n",
    "        \n",
    "        print(\"\\n[INFO] Auto threshold mode enabled.\")\n",
    "        pos_idx = [i for i, l in enumerate(labels) if l == 1]\n",
    "        neg_idx = [i for i, l in enumerate(labels) if l == 0]\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(pos_idx);  np.random.shuffle(neg_idx)\n",
    "        # # ------------ version 1: 20 % calibrate(pos:neg ~= 1:1), 80 % infer ------------\n",
    "        # cap = int(0.2 * len(labels))                      \n",
    "        # n_pos_calib = min(len(pos_idx), cap // 2)         \n",
    "        # n_neg_calib = min(len(neg_idx), cap - n_pos_calib)  \n",
    "        # calib_idx   = pos_idx[:n_pos_calib] + neg_idx[:n_neg_calib]  \n",
    "        # np.random.shuffle(calib_idx)                     \n",
    "\n",
    "        # calib_pairs  = [pairs[i]  for i in calib_idx]\n",
    "        # calib_labels = [labels[i] for i in calib_idx]\n",
    "\n",
    "        # print(f\"[Calib] Positive={sum(calib_labels)} | Negative={len(calib_labels)-sum(calib_labels)}\")\n",
    "        # threshold, f1_calib = auto_calibrate_threshold(model, calib_pairs, calib_labels, device)\n",
    "        # print(f\"Auto-calibrated threshold = {threshold:.4f}  (F1 on calib = {f1_calib:.4f})\")\n",
    "\n",
    "        # # ------------ version 2: 20 % calibrate (pos:neg ~= 1:10) 100% infer ------------\n",
    "        # === Keep the real distribution as original inference data ===\n",
    "        total_pairs = len(labels)\n",
    "        pos_ratio = len(pos_idx) / total_pairs\n",
    "        neg_ratio = len(neg_idx) / total_pairs\n",
    "\n",
    "        cap = int(0.12 * total_pairs)\n",
    "        n_pos_calib = int(cap * pos_ratio)\n",
    "        n_neg_calib = cap - n_pos_calib\n",
    "\n",
    "        n_pos_calib = min(n_pos_calib, len(pos_idx))\n",
    "        n_neg_calib = min(n_neg_calib, len(neg_idx))\n",
    "\n",
    "        calib_idx = pos_idx[:n_pos_calib] + neg_idx[:n_neg_calib]\n",
    "        np.random.shuffle(calib_idx)\n",
    "\n",
    "        calib_pairs  = [pairs[i]  for i in calib_idx]\n",
    "        calib_labels = [labels[i] for i in calib_idx]\n",
    "\n",
    "        print(f\"[Calib] Positive={sum(calib_labels)} | Negative={len(calib_labels)-sum(calib_labels)}\")\n",
    "        threshold, f1_calib = auto_calibrate_threshold(model, calib_pairs, calib_labels, device)\n",
    "        print(f\"Auto-calibrated threshold = {threshold:.4f}  (F1 on calib = {f1_calib:.4f})\")\n",
    "\n",
    "        \n",
    "        # ------------ final inference ------------\n",
    "        n_pos = sum(labels)                  # label == 1\n",
    "        n_neg = len(labels) - n_pos          # label == 0\n",
    "        print(f\"[All pairs] Positive={n_pos}  |  Negative={n_neg}  \"\n",
    "            f\"({n_pos/len(labels):.2%} positive)\")\n",
    "        inference_batch(model, pairs, labels,\n",
    "                    threshold=threshold, device=device, desc=\"Infer-100%\")\n",
    "        print(f\"[Inference] Total pairs: {len(pairs)}\")\n",
    "    \n",
    "    elif args.thresholds:\n",
    "        print(\"\\n[INFO] Manual threshold mode enabled.\")\n",
    "        for t in args.thresholds:\n",
    "            print(f\"\\n=== threshold {t} ===\")\n",
    "            inference_batch(model, pairs, labels, t, device, desc=f\"@{t}\")\n",
    "\n",
    "    else:\n",
    "        print(\"ERROR: You must either use --auto_threshold or provide --thresholds values.\")\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7e503",
   "metadata": {},
   "source": [
    "# utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913d64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, pairs_file, root_dir=\"\"):\n",
    "        data = np.load(pairs_file, allow_pickle=True)\n",
    "        self.pairs = data[\"pairs\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path = self.pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if isinstance(img1_path, str):\n",
    "            img1_full_path = os.path.join(self.root_dir, img1_path)\n",
    "            img1 = np.load(img1_full_path)\n",
    "        else:\n",
    "            img1 = img1_path  \n",
    "\n",
    "        if isinstance(img2_path, str):\n",
    "            img2_full_path = os.path.join(self.root_dir, img2_path)\n",
    "            img2 = np.load(img2_full_path)\n",
    "        else:\n",
    "            img2 = img2_path\n",
    "\n",
    "        img1 = torch.tensor(img1).unsqueeze(0)  # [1, H, W]\n",
    "        img2 = torch.tensor(img2).unsqueeze(0)\n",
    "        label = torch.tensor(label).float()\n",
    "\n",
    "        return img1, img2, label\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):          \n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, out1, out2, label):\n",
    "        # Euclidean distance between embeddings\n",
    "        d = torch.nn.functional.pairwise_distance(out1, out2)\n",
    "        # label = 1 → same finger  → target distance = 0\n",
    "        # label = 0 → different    → target distance ≥ margin\n",
    "        loss = label * d.pow(2) + (1 - label) * torch.clamp(self.margin - d, min=0.0).pow(2)\n",
    "        return loss.mean()                  # ← 缩进对齐\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_loss(train_losses, val_losses, save_path=None):\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Loss plot saved to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def save_checkpoint(model, path=\"checkpoints/best_model.pt\"):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_distance_distribution(model, dataloader, device=\"cpu\", save_path=None):\n",
    "    import torch.nn.functional as F\n",
    "    model.eval()\n",
    "\n",
    "    pos_distances = []\n",
    "    neg_distances = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, label in dataloader:\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            out1, out2 = model(img1, img2)\n",
    "            dist = F.pairwise_distance(out1, out2)\n",
    "\n",
    "            for d, l in zip(dist, label):\n",
    "                if l == 1:\n",
    "                    pos_distances.append(d.item())\n",
    "                else:\n",
    "                    neg_distances.append(d.item())\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(pos_distances, bins=50, alpha=0.6, label=\"Positive (same finger)\")\n",
    "    plt.hist(neg_distances, bins=50, alpha=0.6, label=\"Negative (different finger)\")\n",
    "    plt.xlabel(\"Distance\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distance Distribution\")\n",
    "    plt.legend()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\" Distance histogram saved to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curve(model, dataloader, device=\"cpu\", save_path=None):\n",
    "    import torch.nn.functional as F\n",
    "    model.eval()\n",
    "\n",
    "    all_distances = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, label in dataloader:\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            out1, out2 = model(img1, img2)\n",
    "            dist = F.pairwise_distance(out1, out2)\n",
    "\n",
    "            all_distances.extend(dist.cpu().numpy())\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "\n",
    "    # score = -distance\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, -1 * np.array(all_distances)) \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\" ROC curve saved to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def print_classification_report(tp, tn, fp, fn):\n",
    "    total = tp + tn + fp + fn\n",
    "    acc = (tp + tn) / total * 100\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(\"\\n Classification Report:\")\n",
    "    print(f\"  Accuracy  : {acc:.2f}%\")\n",
    "    print(f\"  Precision : {precision:.4f}\")\n",
    "    print(f\"  Recall    : {recall:.4f}\")\n",
    "    print(f\"   F1 Score  : {f1:.4f}\")\n",
    "    print(f\"  TP={tp} | TN={tn} | FP={fp} | FN={fn}\")\n",
    "\n",
    "def plot_metrics_vs_threshold(model, dataloader, device=\"cpu\", save_path=None):\n",
    "    model.eval()\n",
    "    thresholds = np.linspace(0.7, 1.1, 50)\n",
    "    all_distances = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"Extracting embeddings...\")\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, label in tqdm(dataloader, desc=\"Forward pass\"):\n",
    "            img1, img2 = img1.to(device), img2.to(device)\n",
    "            out1, out2 = model(img1, img2)\n",
    "            dist = F.pairwise_distance(out1, out2)\n",
    "            all_distances.extend(dist.cpu().numpy())\n",
    "            all_labels.extend(label.numpy())\n",
    "\n",
    "    all_distances = np.array(all_distances)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "\n",
    "    print(\"Calculating metrics for thresholds...\")\n",
    "    for t in tqdm(thresholds, desc=\"Threshold\"):\n",
    "        preds = (all_distances < t).astype(int)\n",
    "        accuracies.append((preds == all_labels).mean())\n",
    "        precisions.append(precision_score(all_labels, preds, zero_division=0))\n",
    "        recalls.append(recall_score(all_labels, preds, zero_division=0))\n",
    "        f1s.append(f1_score(all_labels, preds, zero_division=0))\n",
    "\n",
    "    best_idx = np.argmax(f1s)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_f1 = f1s[best_idx]\n",
    "    print(f\"\\n Best threshold = {best_threshold:.6f} → F1 Score = {best_f1:.6f}\")\n",
    "\n",
    "    # Plot all metrics\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(thresholds, accuracies, label=\"Accuracy\")\n",
    "    plt.plot(thresholds, precisions, label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls, label=\"Recall\")\n",
    "    plt.plot(thresholds, f1s, label=\"F1 Score\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Metric Value\")\n",
    "    plt.title(\"Metrics vs Threshold\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Metrics plot saved to {save_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    with open(\"./outputs/best_threshold.txt\", \"w\") as f:\n",
    "        f.write(f\"{best_threshold:.6f}\")\n",
    "    print(\"Best threshold saved to outputs/best_threshold.txt\")\n",
    "\n",
    "    return best_threshold\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tif320env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
